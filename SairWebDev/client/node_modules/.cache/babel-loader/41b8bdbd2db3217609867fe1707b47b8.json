{"ast":null,"code":"var _jsxFileName = \"/Users/harshtyagi/Documents/Sair/React2/Middleware/client/src/components/videosegmentation/videosegmentation.js\";\nimport React from \"react\";\nimport axios from \"axios\";\nimport img from \"./image1.jpg\";\nimport * as bodyPix from \"@tensorflow-models/body-pix\";\n\nconst videosegmentation = () => {\n  return React.createElement(\"div\", {\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 8\n    },\n    __self: this\n  }, \"Reached Video Segmentation Page\", React.createElement(\"img\", {\n    src: img,\n    id: \"image\",\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 10\n    },\n    __self: this\n  }), React.createElement(\"button\", {\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 11\n    },\n    __self: this\n  }, \"Click\"));\n};\n\nasync function loadAndUseBodyPix() {\n  const net = await bodyPix.load(); // BodyPix model loaded\n\n  const imageElement = document.getElementById(\"image\"); // load the BodyPix model from a checkpoint\n  // arguments for estimating person segmentation.\n\n  const outputStride = 16;\n  const segmentationThreshold = 0.5;\n  const personSegmentation = await net.estimatePersonSegmentation(imageElement, outputStride, segmentationThreshold); // console.log(personSegmentation);\n}\n\nexport default videosegmentation;","map":{"version":3,"sources":["/Users/harshtyagi/Documents/Sair/React2/Middleware/client/src/components/videosegmentation/videosegmentation.js"],"names":["React","axios","img","bodyPix","videosegmentation","loadAndUseBodyPix","net","load","imageElement","document","getElementById","outputStride","segmentationThreshold","personSegmentation","estimatePersonSegmentation"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAOC,KAAP,MAAkB,OAAlB;AACA,OAAOC,GAAP,MAAgB,cAAhB;AACA,OAAO,KAAKC,OAAZ,MAAyB,6BAAzB;;AAEA,MAAMC,iBAAiB,GAAG,MAAM;AAC9B,SACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wCAEE;AAAK,IAAA,GAAG,EAAEF,GAAV;AAAe,IAAA,EAAE,EAAC,OAAlB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAFF,EAGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,aAHF,CADF;AAOD,CARD;;AASA,eAAeG,iBAAf,GAAmC;AACjC,QAAMC,GAAG,GAAG,MAAMH,OAAO,CAACI,IAAR,EAAlB,CADiC,CAEjC;;AACA,QAAMC,YAAY,GAAGC,QAAQ,CAACC,cAAT,CAAwB,OAAxB,CAArB,CAHiC,CAKjC;AAEA;;AACA,QAAMC,YAAY,GAAG,EAArB;AACA,QAAMC,qBAAqB,GAAG,GAA9B;AAEA,QAAMC,kBAAkB,GAAG,MAAMP,GAAG,CAACQ,0BAAJ,CAC/BN,YAD+B,EAE/BG,YAF+B,EAG/BC,qBAH+B,CAAjC,CAXiC,CAgBjC;AACD;;AAED,eAAeR,iBAAf","sourcesContent":["import React from \"react\";\nimport axios from \"axios\";\nimport img from \"./image1.jpg\";\nimport * as bodyPix from \"@tensorflow-models/body-pix\";\n\nconst videosegmentation = () => {\n  return (\n    <div>\n      Reached Video Segmentation Page\n      <img src={img} id=\"image\" />\n      <button>Click</button>\n    </div>\n  );\n};\nasync function loadAndUseBodyPix() {\n  const net = await bodyPix.load();\n  // BodyPix model loaded\n  const imageElement = document.getElementById(\"image\");\n\n  // load the BodyPix model from a checkpoint\n\n  // arguments for estimating person segmentation.\n  const outputStride = 16;\n  const segmentationThreshold = 0.5;\n\n  const personSegmentation = await net.estimatePersonSegmentation(\n    imageElement,\n    outputStride,\n    segmentationThreshold\n  );\n  // console.log(personSegmentation);\n}\n\nexport default videosegmentation;\n"]},"metadata":{},"sourceType":"module"}